# -*- coding: utf-8 -*-
"""AI Resume Screener & Job Match System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CJybbOVbIwpelkswT68KlbHA03adRZ9C
"""



"""## Load and explore data

### Subtask:
Load the provided CSV data and perform initial exploration to understand its structure and content.

**Reasoning**:
Import pandas, load the CSV file, display the head, info, and describe the dataframe to fulfill the initial exploration steps.
"""

import pandas as pd

# Read the CSV file into a pandas DataFrame
df = pd.read_csv("/content/AI_Resume_Screening.csv")

# Display the first 5 rows of the DataFrame
display(df.head())

# Print the column names and their data types
display(df.info())

# Display basic descriptive statistics for numerical columns
display(df.describe())

"""## Preprocess text data

### Subtask:
Clean and preprocess the text data from the resumes, which may include removing stop words, punctuation, and converting text to lowercase.

**Reasoning**:
Select the 'Skills' column for preprocessing and apply lowercase conversion, punctuation removal, and stop word removal.
"""

import re
import nltk
from nltk.corpus import stopwords

# Download stopwords if not already downloaded
try:
    stopwords.words('english')
except LookupError:
    nltk.download('stopwords')

# Select the 'Skills' column
text_column = 'Skills'

# Convert to lowercase
df[text_column] = df[text_column].str.lower()

# Remove punctuation
df[text_column] = df[text_column].apply(lambda x: re.sub(r'[^\w\s]', '', x))

# Remove stop words
stop_words = set(stopwords.words('english'))
df[text_column] = df[text_column].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))

# Display the first few rows with the preprocessed text
display(df.head())

"""## Choose and implement embedding model

### Subtask:
Select an appropriate embedding model (e.g., BERT, OpenAI embeddings) and implement the code to generate embeddings for the resume text.

**Reasoning**:
Install the `sentence-transformers` library as it is required to use the `all-MiniLM-L6-v2` model for generating embeddings.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install sentence-transformers

"""**Reasoning**:
Import the SentenceTransformer model and generate embeddings for the 'Skills' column, storing them in a new column.


"""

from sentence_transformers import SentenceTransformer

# Load the pre-trained model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate embeddings for the 'Skills' column
df['skill_embeddings'] = df['Skills'].apply(lambda x: model.encode(x))

# Display the first few rows with the new embeddings column
display(df.head())

"""## Develop skill extraction logic

### Subtask:
Create a method to extract relevant skills from the resume text based on the generated embeddings or other NLP techniques.

**Reasoning**:
Define a function to extract relevant skills using cosine similarity and apply it to the DataFrame.
"""

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def extract_relevant_skills(skills_text, skill_embeddings, known_skills_list, known_skills_embeddings, similarity_threshold=0.7):
    """
    Extracts relevant skills from a resume's skill text using embedding similarity.

    Args:
        skills_text (str): The preprocessed string of skills from a resume.
        skill_embeddings (np.ndarray): The embedding vector for the skills_text.
        known_skills_list (list): A list of known skill terms.
        known_skills_embeddings (np.ndarray): Embeddings for the known_skills_list.
        similarity_threshold (float): The cosine similarity threshold for matching.

    Returns:
        list: A list of extracted relevant skills.
    """
    extracted_skills = []
    # Split the skills text into individual skill terms (simple split for demonstration)
    resume_skill_terms = skills_text.split()

    # Compare each term in the resume skills text to the known skills
    for term in resume_skill_terms:
        # Encode the individual term (simple approach, could be improved)
        term_embedding = model.encode(term).reshape(1, -1)

        # Calculate cosine similarity with known skills embeddings
        similarities = cosine_similarity(term_embedding, known_skills_embeddings)

        # Find the best match among known skills
        best_match_index = np.argmax(similarities)
        best_match_similarity = similarities[0, best_match_index]

        # If the best match is above the threshold and the term is not already added
        if best_match_similarity > similarity_threshold and known_skills_list[best_match_index] not in extracted_skills:
             # Add the known skill that best matches the term
            extracted_skills.append(known_skills_list[best_match_index])
        elif term not in extracted_skills and term in known_skills_list:
             # If the term itself is a known skill, add it directly
             extracted_skills.append(term)


    return extracted_skills

# Create a list of unique skills from the 'Skills' column to serve as known skills
# This is a simplified approach; a real-world scenario might use a curated list
all_skills = df['Skills'].str.split().explode().unique()
known_skills_list = list(all_skills)

# Generate embeddings for the known skills
known_skills_embeddings = model.encode(known_skills_list)

# Apply the function to each row
df['extracted_skills'] = df.apply(lambda row: extract_relevant_skills(
    row['Skills'],
    row['skill_embeddings'],
    known_skills_list,
    known_skills_embeddings
), axis=1)

# Display the first few rows with the new 'extracted_skills' column
display(df[['Skills', 'extracted_skills']].head())

"""## Implement job matching logic

### Subtask:
Develop an algorithm to match candidates to job descriptions based on the similarity of their skill sets or resume embeddings.

**Reasoning**:
I need to define a job description or skills, generate embeddings for it, calculate similarity with candidate embeddings, store the scores, rank candidates, and display the top N. This can be done in one code block.
"""

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# 1. Define a sample job description or set of required skills
job_description = "We are looking for a data scientist with strong skills in machine learning, deep learning, python, and tensorflow."
required_skills = ["machine learning", "deep learning", "python", "tensorflow"]

# Convert required_skills to a single string for embedding
required_skills_text = " ".join(required_skills)

# 2. Generate embeddings for the job description or required skills
# Use the same model as used for resume embeddings
job_embedding = model.encode(required_skills_text).reshape(1, -1)

# 3. Calculate the similarity between the job embeddings and the skill embeddings of each candidate
# Ensure 'skill_embeddings' are in a suitable format (e.g., numpy array)
candidate_skill_embeddings = np.vstack(df['skill_embeddings'].values)

# Calculate cosine similarity between the job embedding and all candidate skill embeddings
similarity_scores = cosine_similarity(job_embedding, candidate_skill_embeddings)[0]

# 4. Create a new column in the DataFrame to store the similarity scores
df['similarity_score'] = similarity_scores

# 5. Rank candidates based on their similarity scores to the job requirements
ranked_candidates = df.sort_values(by='similarity_score', ascending=False)

# 6. Display the top N candidates with the highest similarity scores
N = 10 # You can adjust N to display a different number of top candidates
display(ranked_candidates[['Name', 'Skills', 'Experience (Years)', 'similarity_score']].head(N))

"""## Build a simple web application (optional)

### Subtask:
If desired, create a basic web application using Streamlit or FastAPI to demonstrate the functionality of the tool.

**Reasoning**:
Create a Streamlit application script to demonstrate the functionality of the tool, including loading data, creating a UI for job description input and N selection, implementing matching logic, and displaying results.
"""

import streamlit as st
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import pandas as pd

# Load the pre-trained model (ensure this model is the same as used previously)
@st.cache_resource
def load_model():
    return SentenceTransformer('all-MiniLM-L6-v2')

model = load_model()

# Load the DataFrame (ensure this path is correct or the df is available)
# In a real app, you might load this from a database or file
# For this example, we'll assume df is already loaded in the environment
# If not, you would load it here:
# df = pd.read_csv("/content/AI_Resume_Screening.csv")
# Preprocess skills and generate embeddings if not already done
# This part assumes the preprocessing and embedding generation from previous steps
# are either already applied to the loaded df or will be done here.
# For demonstration, we'll assume df already has 'skill_embeddings'.

# App Title
st.title("AI Resume Screening and Matching Tool")

# User Input: Job Description/Required Skills
job_description_input = st.text_area("Enter the job description or required skills:", "")

# User Input: Number of Top Candidates
N_input = st.number_input("Number of top candidates to display:", min_value=1, value=10, step=1)

# Button to trigger matching
if st.button("Find Matching Candidates"):
    if job_description_input:
        # 1. Generate embeddings for the input job requirements
        job_embedding = model.encode(job_description_input).reshape(1, -1)

        # 2. Calculate similarity between job embeddings and candidate skill embeddings
        # Ensure 'skill_embeddings' are in a suitable format (e.g., numpy array)
        # Handle potential errors if 'skill_embeddings' column is missing or empty
        if 'skill_embeddings' in df.columns and not df['skill_embeddings'].empty:
            try:
                candidate_skill_embeddings = np.vstack(df['skill_embeddings'].values)
                similarity_scores = cosine_similarity(job_embedding, candidate_skill_embeddings)[0]

                # 3. Add the similarity scores to the DataFrame
                df['similarity_score'] = similarity_scores

                # 4. Rank candidates based on their similarity scores
                ranked_candidates = df.sort_values(by='similarity_score', ascending=False).head(N_input)

                # 5. Display the top N candidates
                st.subheader(f"Top {N_input} Matching Candidates:")
                if not ranked_candidates.empty:
                    # Select relevant columns for display
                    display_cols = ['Name', 'Skills', 'Experience (Years)', 'Education', 'Job Role', 'similarity_score']
                    # Ensure display_cols are in the dataframe
                    display_df = ranked_candidates[[col for col in display_cols if col in ranked_candidates.columns]]
                    st.dataframe(display_df)
                else:
                    st.write("No candidates found.")

            except ValueError as e:
                st.error(f"Error processing embeddings: {e}. Please ensure 'skill_embeddings' are correctly generated.")
            except Exception as e:
                st.error(f"An unexpected error occurred: {e}")
        else:
            st.error("Skill embeddings not found in the DataFrame. Please ensure the data is loaded and processed correctly.")
    else:
        st.warning("Please enter a job description or required skills.")

# Instructions on how to run the app (for the user)
st.markdown("""
### How to Run This App:

1.  Save the code above as a Python file (e.g., `app.py`).
2.  Make sure you have Streamlit installed (`pip install streamlit`).
3.  Open your terminal or command prompt.
4.  Navigate to the directory where you saved `app.py`.
5.  Run the command: `streamlit run app.py`
6.  The app will open in your web browser.
""")

"""**Reasoning**:
The previous command failed because the `streamlit` library is not installed. Install `streamlit` using pip.


"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit

"""**Reasoning**:
Now that `streamlit` is installed, rerun the code to create the Streamlit application script and define the UI and matching logic. This will complete the core task of creating the web application.


"""

import streamlit as st
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import pandas as pd

# Load the pre-trained model (ensure this model is the same as used previously)
@st.cache_resource
def load_model():
    return SentenceTransformer('all-MiniLM-L6-v2')

model = load_model()

# Load the DataFrame (ensure this path is correct or the df is available)
# In a real app, you might load this from a database or file
# For this example, we'll assume df is already loaded in the environment
# If not, you would load it here:
# df = pd.read_csv("/content/AI_Resume_Screening.csv")
# Preprocess skills and generate embeddings if not already done
# This part assumes the preprocessing and embedding generation from previous steps
# are either already applied to the loaded df or will be done here.
# For demonstration, we'll assume df already has 'skill_embeddings'.

# App Title
st.title("AI Resume Screening and Matching Tool")

# User Input: Job Description/Required Skills
job_description_input = st.text_area("Enter the job description or required skills:", "")

# User Input: Number of Top Candidates
N_input = st.number_input("Number of top candidates to display:", min_value=1, value=10, step=1)

# Button to trigger matching
if st.button("Find Matching Candidates"):
    if job_description_input:
        # 1. Generate embeddings for the input job requirements
        job_embedding = model.encode(job_description_input).reshape(1, -1)

        # 2. Calculate similarity between job embeddings and candidate skill embeddings
        # Ensure 'skill_embeddings' are in a suitable format (e.g., numpy array)
        # Handle potential errors if 'skill_embeddings' column is missing or empty
        if 'skill_embeddings' in df.columns and not df['skill_embeddings'].empty:
            try:
                candidate_skill_embeddings = np.vstack(df['skill_embeddings'].values)
                similarity_scores = cosine_similarity(job_embedding, candidate_skill_embeddings)[0]

                # 3. Add the similarity scores to the DataFrame
                df['similarity_score'] = similarity_scores

                # 4. Rank candidates based on their similarity scores
                ranked_candidates = df.sort_values(by='similarity_score', ascending=False).head(N_input)

                # 5. Display the top N candidates
                st.subheader(f"Top {N_input} Matching Candidates:")
                if not ranked_candidates.empty:
                    # Select relevant columns for display
                    display_cols = ['Name', 'Skills', 'Experience (Years)', 'Education', 'Job Role', 'similarity_score']
                    # Ensure display_cols are in the dataframe
                    display_df = ranked_candidates[[col for col in display_cols if col in ranked_candidates.columns]]
                    st.dataframe(display_df)
                else:
                    st.write("No candidates found.")

            except ValueError as e:
                st.error(f"Error processing embeddings: {e}. Please ensure 'skill_embeddings' are correctly generated.")
            except Exception as e:
                st.error(f"An unexpected error occurred: {e}")
        else:
            st.error("Skill embeddings not found in the DataFrame. Please ensure the data is loaded and processed correctly.")
    else:
        st.warning("Please enter a job description or required skills.")

# Instructions on how to run the app (for the user)
st.markdown("""
### How to Run This App:

1.  Save the code above as a Python file (e.g., `app.py`).
2.  Make sure you have Streamlit installed (`pip install streamlit`).
3.  Open your terminal or command prompt.
4.  Navigate to the directory where you saved `app.py`.
5.  Run the command: `streamlit run app.py`
6.  The app will open in your web browser.
""")

"""## Summary:

### Data Analysis Key Findings

*   The dataset contains 1000 entries and 11 columns, including 'Skills', 'Experience (Years)', 'Job Role', and 'AI Score (0-100)'.
*   The 'Certifications' column has a significant number of missing values (726 out of 1000).
*   Text preprocessing on the 'Skills' column involved converting text to lowercase, removing punctuation, and eliminating English stop words.
*   The `all-MiniLM-L6-v2` Sentence Transformer model was successfully used to generate embeddings for the preprocessed 'Skills' data, stored in a new 'skill\_embeddings' column.
*   A function was developed to extract relevant skills by comparing individual skill terms from resumes against a list of known skills using cosine similarity of their embeddings.
*   A job matching algorithm was implemented by calculating the cosine similarity between the embedding of a sample job description/required skills and the 'skill\_embeddings' of each candidate.
*   Candidates were ranked based on their similarity scores to the job requirements, and the top candidates were identified.
*   A basic Streamlit web application was outlined to provide a user interface for the job matching functionality, allowing users to input job descriptions and view ranked candidates.

### Insights or Next Steps

*   The current skill extraction logic, which uses terms within the existing 'Skills' column as "known skills," is a simplified approach. A more robust method could involve using a curated, external list of skills or employing techniques like Named Entity Recognition (NER) to identify skills not explicitly listed but present in the resume text.
*   The Streamlit application provides a basic demonstration. For a production-ready tool, consider adding features like uploading resume files, parsing different resume formats (PDF, DOCX), handling multiple job descriptions, and incorporating other factors beyond skills (e.g., experience, education, certifications) into the matching algorithm.

"""